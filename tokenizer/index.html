
<!DOCTYPE html>
<html lang="en-us">
<head>
<meta charset="utf-8">
<meta name="generator" content="Hugo 0.16-DEV" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<link rel="stylesheet" href="../css/normalize.css">
<link rel="stylesheet" href="../css/skeleton.css">
<link rel="stylesheet" href="../css/custom.css">
<link rel="alternate" href="index.xml" type="application/rss+xml" title="Muzic">
<link rel="shortcut icon" href="favicon.png" type="image/x-icon" />
<title>The MusicLang tokenizer : An enriched musical encoding</title>
</head>
<body>

<div class="container">
	<header role="banner">
	</header>
	<main role="main">
		<article itemscope itemtype="https://schema.org/BlogPosting">
            <h1 class="entry-title" itemprop="headline">The MusicLang tokenizer : Toward controllable symbolic music generation</h1>
			<section itemprop="entry-text">
                <h2 id="abstract">Abstract</h2>
                <p>In this article, we introduce the encoding mechanism of musical scores when using the MusicLang model.
                    We will detail the tokenization scheme and demonstrate its capacity to afford users profound control over the musical content generated by transformer models.
                </p>
                <h2 id="introduction">Introduction</h2>
                <p>
                    The tokenization for symbolic music generation has been the subject of many investigation.
                    Prior research has predominantly concentrated on optimizing loss functions related to the next token prediction problem.
                    Our research, however, focuses on the controllability of the output sequence, a critical aspect for its application in music composition assistance.
                    This facet of controllability has been less frequently addressed, possibly due to the challenge of quantifying an universal metric for controllability performance.
                    In our novel tokenization scheme, we employ an in-depth analysis of the musical score to distill pertinent musical information, which is then encoded within the tokenization process.
                    Such comprehensive analysis enhances the capacity to finely tune the outputs of music language models trained using this approach.
                    Furthermore, we introduce a controlled generation framework designed to forecast music incorporating these control constraints.
                    The methodologies for tokenization and prediction have been encapsulated within an open-source initiative named <a href="https://github.com/musiclang/musiclang_predict">musiclang_predict</a>.
                </p>

                <h2 id="musical-representation">How to represent a score ?</h2>

                <div style="text-align:center"><img src="../images/tokenizer/vertical_horizontal.png" width=1000 height=1000>
                    <p>Figure 1: Horizontal VS Vertical parsing</p>
                </div>
                <p>
                    When representing music with tokens, various parsing strategies can be employed, each presenting distinct advantages and disadvantages.
                    A primary challenge is the simultaneous occurrence of musical events.
                    Deciding the sequence in which to parse these events introduces a fundamental dilemma, a trade-off between melody or harmony representation.
                    Below, we offer a comparison of these methods:
                </p>
                <ol>
                    <li><b>Parse the music vertically</b>, meaning notes are fully ordered timewise. It favours harmonically rich generation.</li>
                    <li><b>Parse the music horizontally</b>, meaning notes are grouped by voice. It favours melodic generation.</li>
                    <li><b>Combine both methods</b>: For instance, order notes by time within each measure, but overall, arrange them by musical part. There are many ways to do this.
                    </li>
                </ol>
                <p>Most tools designed for creating long music sequences opt for a mixed method. This reflects the way musicians typically read music:</p>
                <ul>
                    <li>Measures act as key vertical breaks in music.</li>
                    <li>Within a measure, instruments or voices are usually considered horizontally, yet in connection with each other.</li>
                    <li>On paper, notes from the same instrument are often divided into distinct, meaningful parts.</li>
                </ul>
                <p>It seems we generally interpret music in a combined manner, vertically within horizontal sections of a single measure.</p>

                <div style="text-align:center"><img src="../images/tokenizer/hybrid_way.png" width=1000 height=1000>
                    <p>Figure 2: A natural way to process music</p>
                </div>

                <p>This "natural" approach to music representation guides the structure of our tokenizer: We depict music <b>measure by measure</b>.
                    Within each measure, music is presented as a <b>list of melodies</b>, with each melody embodying a <b>single-voice</b> line from an instrument.</p>


                <h2 id="tokenization-scheme">Tokenization overview</h2>

                <div style="text-align:center"><img src="../images/tokenizer/tokenizer_overview.png" width=1000 height=1000>
                    <p>Figure 3: High level tokenization scheme</p>
                </div>

                 <p>The tokenization scheme we propose consists of the following steps:</p>
                <ul>
                    <li>Chord scale tokenization</li>
                    <li>Melody control tokenization</li>
                    <li>Melody tokenization</li>
                </ul>

                <p>Let's delve into each module.</p>


                <h3 id="chord-scale-tokenization">Chord scale tokenization</h3>

                <div style="text-align:center"><img src="../images/tokenizer/chord_tokenization.png" width=1000 height=1000>
                    <p>Figure 4: Chord scale tokenization scheme</p>
                </div>

                <p>Each chord is depicted using seven tokens (six control tokens plus one "CHORD_CHANGE" token):</p>
                <ul>
                    <li><b>Scale degrees</b>: I, II, III, IV, V, VI, VII - represent the chord's degree within the scale. This does not specify the chord's type; the mode and extension are needed for that.</li>
                    <li><b>Tonality root</b>: 0 to 12, corresponding to each pitch class possible (C to B).</li>
                    <li><b>Tonality mode</b>: m for harmonic minor, M for major.</li>
                    <li><b>Chord octave</b>: -4 to 4, indicating the root note's octave relative to the 4th octave.</li>
                    <li><b>Extension</b>: Uses figured bass notation to detail the chord's bass and any extensions (e.g., '' (triad in root position), '6', '64', '7', '65', '43', '2'), including variations with sus2 or sus4 for suspended chords.</li>
                    <li><b>Duration numerator/denominator</b>: Numerical values representing the chord's fractional duration, providing control over the time signature (since one chord equals one bar in MusicLang).</li>
                </ul>

                <p>
                    Please note that we are not only representing the current chord for each measure, but also the scale in which the chord belongs.
                    It is very important because notes are also represented in the context of the current chord/scale root note.
                    Finally we provide chord extension to be able to represent common figured bass notation. It will tell the model which notes are
                    statistically more likely to be played in the context of the current chord and also what is the bass note of the chord.
                </p>

                <h3 id="voice-control-tokenization">Melody control tokenization</h3>

                <div style="text-align:center"><img src="../images/tokenizer/voice_control.png" width=1000 height=1000>
                    <p>Figure 5: Melody (or voice) control scheme</p>
                </div>

                <p>
                    In addition to control the chord progression and scale, we also provide a way to control the melody of each voice.
                    This is done by providing a serie of token for each voice of each instrument, namely a note density (average number of notes per quarter)
                    a note mean octave relative to the fourth octave, an average velocity in musical notation.

                    Depending on the model version we also provide a token for voice index of the given instrument.

                </p>

                <h3 id="note-tokenization">Note tokenization</h3>

                <div style="text-align:center"><img src="../images/tokenizer/note_tokenization.png" width=1000 height=1000>
                    <p>Figure 6: Note tokens</p>
                </div>

                <p>
                    The scheme's primary innovation and inherent bias lies in its approach to pitch representation,
                    which is relative to both the root note of a chord and a corresponding scale.
                    For instance, in the context of a V chord in the key of C major (G major),
                    the root note is denoted as <b>s0</b>. The second note in the chord's scale, A, is labeled as <b>s1</b>,
                    and the sequence continues in this manner. Notes that are not part of the scale are marked with an 'h'.
                    For example, <b>h1</b> would represent an Ab in our V chord of C major.
                    This method allows for a pitch to be understood in terms of its harmonic function and scale position.
                    It gives to the model a direct "harmonic" values while generating melodies.
                </p>

                <h2 id="test-the-tokenization">Test the tokenization</h2>
                <p> Now, we implemented this tokenizer to be compatible with the HuggingFace API.
                    With <a href="https://github.com/musiclang/musiclang_predict">musiclang_predict</a> you can easily test the tokenization on your own midi scores,
                    and train your own model using this tokenization scheme.
                    Here is a simple exemple of how to use the tokenizer to encode a midi file :
                </p>

<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0 0 50px 0; line-height: 125%"><span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">musiclang_predict</span> <span style="color: #008800; font-weight: bold">import</span> MusicLangTokenizer
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">musiclang</span> <span style="color: #008800; font-weight: bold">import</span> Score
<span style="color: #888888"># Load model and tokenizer, we use the v1 of the musiclang model for this purpose</span>
midi_file <span style="color: #333333">=</span> <span style="background-color: #fff0f0">&#39;path_to_your_midi_file.mid&#39;</span>
score <span style="color: #333333">=</span> Score<span style="color: #333333">.</span>from_midi(midi_file)
tokenizer <span style="color: #333333">=</span> MusicLangTokenizer(<span style="background-color: #fff0f0">&#39;musiclang/musiclang-4k&#39;</span>)
tokens <span style="color: #333333">=</span> tokenizer<span style="color: #333333">.</span>tokenize(score)
<span style="color: #008800; font-weight: bold">print</span>(tokens)
</pre>
</div>


                <p style="margin-top:20px">You will get a list of tokens formatted like this : ['CHORD_CHANGE', 'CHORD_DEGREE__4', 'TONALITY_DEGREE__5', 'TONALITY_MODE__M', 'CHORD_OCTAVE__-1', 'CHORD_EXTENSION__', 'CHORD_DURATION_NUM__4', 'CHORD_DURATION_DEN__1', 'INSTRUMENT_NAME__harpsichord', 'DENSITY__medium', 'AVERAGE_OCTAVE__0', 'AMPLITUDE__ff', 'NOTE_TYPE__r', 'NOTE_VAL__0', 'NOTE_OCTAVE__0', 'NOTE_AMP__mf', 'NOTE_DURATION_NUM__1', 'NOTE_DURATION_DEN__1', 'NOTE_TYPE__s', 'NOTE_VAL__4', 'NOTE_OCTAVE__0', 'NOTE_AMP__ff' ...</p>

                <p>But you can also do some other interesting task like extracting a "song template" from a midi file. What we mean by a song template is that we only
                keep the control token out of the tokenization and we format it in a human readable JSON format. Here is an example of how to do it :</p>

<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">musiclang_predict</span> <span style="color: #008800; font-weight: bold">import</span> midi_file_to_template, predict_with_template, MusicLangTokenizer
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">musiclang</span> <span style="color: #008800; font-weight: bold">import</span> Score
<span style="color: #008800; font-weight: bold">import</span> <span style="color: #0e84b5; font-weight: bold">json</span>
<span style="color: #888888"># Load model and tokenizer, we use the v1 of the musiclang model for this purpose</span>
midi_file <span style="color: #333333">=</span> <span style="background-color: #fff0f0">&#39;path_to_your_midi_file.mid&#39;</span>
<span style="color: #888888"># For now this midi2template is independent of the tokenizer, it will only extract all possible control tokens from the midi file</span>
template <span style="color: #333333">=</span> midi_file_to_template(midi_file)
<span style="color: #008800; font-weight: bold">with</span> <span style="color: #007020">open</span>(<span style="background-color: #fff0f0">&#39;template.json&#39;</span>, <span style="background-color: #fff0f0">&#39;w&#39;</span>) <span style="color: #008800; font-weight: bold">as</span> f:
    json<span style="color: #333333">.</span>dump(template, f, indent<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">4</span>)
</pre></div>


                <p style="margin-top:20px">The content of your file will look like this : </p>
                <pre>
                    <code class="json">

{
    "tonality": "e",
    "tempo": 130,
    "time_signature": [
        4,
        4
    ],
    "chords": [
        {
            "orchestration": [
                {
                    "instrument_name": "acoustic_bass",
                    "instrument_voice": 0,
                    "amplitude": "mf",
                    "octave": 0,
                    "density": 0.5
                },
                {
                    "instrument_name": "drums_0",
                    "instrument_voice": 0,
                    "amplitude": "mf",
                    "octave": -2,
                    "density": 0.75
                },
                {
                    "instrument_name": "acoustic_guitar",
                    "instrument_voice": 0,
                    "amplitude": "mf",
                    "octave": 1,
                    "density": 0.5
                },
                {
                    "instrument_name": "acoustic_guitar",
                    "instrument_voice": 1,
                    "amplitude": "mf",
                    "octave": 1,
                    "density": 0.5
                },
                {
                    "instrument_name": "acoustic_guitar",
                    "instrument_voice": 2,
                    "amplitude": "mf",
                    "octave": 0,
                    "density": 0.5
                }
            ],
            "chord": "i64"
        },
        {
            "orchestration": [
                {
                    "instrument_name": "acoustic_bass",
                    "instrument_voice": 0,
                    "amplitude": "mf",
                    "octave": 0,
                    "density": 0.25
                },
                {
                    "instrument_name": "drums_0",
                    "instrument_voice": 0,
                    "amplitude": "mf",
                    "octave": -2,
                    "density": 1.0
                },
                {
                    "instrument_name": "acoustic_guitar",
                    "instrument_voice": 0,
                    "amplitude": "mf",
                    "octave": 1,
                    "density": 0.75
                },
                {
                    "instrument_name": "acoustic_guitar",
                    "instrument_voice": 1,
                    "amplitude": "mf",
                    "octave": 1,
                    "density": 0.75
                },
                {
                    "instrument_name": "acoustic_guitar",
                    "instrument_voice": 2,
                    "amplitude": "mf",
                    "octave": 0,
                    "density": 0.75
                }
            ],
            "chord": "i64"
        }
    ]
}
                    </code>
                </pre>

                <p> This template is a nice summary of a song, defining its chord progression and outlining the melodic characteristics of each bars.
                From this base we can imagine a model trained to generate these template from a succinct prompt, or to generate a full song from a template.
                </p>

                <h2 id="conclusion">Conclusion & Next steps</h2>

                <p>
                Controllability is crucial for the practical application of music generated by music language models.
                    We have developed a tokenization scheme that allows users to manipulate various score parameters,
                    such as chord and scale progressions, and even exert melodic influence over individual voices within each instrument.
                    The effectiveness of this tokenization is contingent upon the accuracy of the score analysis, introducing a significant
                    inductive bias to the trained model, namely the chord scale detection and the voice separation algorithm.
                    By training a transformer model on this tokenization framework, we demonstrate the feasibility of generating music that
                    adheres closely to a predefined "song template." To experiment with these open-source models, please visit MusicLang Predict.
                    Moreover, this approach paves the way for a prompt-based music generation model. This could involve an intermediate model
                    trained to produce the appropriate control tokens in response to a specified musical prompt.
                </p>

			</section>
		</article>
	</main>

</div>

<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-139981676-1', 'auto');
	ga('send', 'pageview');
</script>

<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         HTML: ["input/TeX","output/HTML-CSS"],
         TeX: {
                Macros: {
                         bm: ["\\boldsymbol{#1}", 1],
                         argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                         argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
                extensions: ["AMSmath.js","AMSsymbols.js"],
                equationNumbers: { autoNumber: "AMS" } },
         extensions: ["tex2jax.js"],
         jax: ["input/TeX","output/HTML-CSS"],
         tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true },
         "HTML-CSS": { availableFonts: ["TeX"],
                       linebreaks: { automatic: true } }
     });
 </script>

 <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
       }
     });
 </script>

 <script type="text/javascript" async
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
 </script>




</body>
</html>
